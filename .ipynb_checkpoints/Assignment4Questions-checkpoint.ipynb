{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4\n",
    "\n",
    "Now that we are finally learning about Bayesian optimization policies, it is a good idea to gain experience with actual use-cases of Bayesian/bandit optimization. In this problem, we will be tuning the hyperparameters of a CNN on the MNIST dataset using Ax (https://ax.dev/). Ax is a platform for optimizing experiements using multi-armed bandits and Bayesian optimization. Ax is built on BoTorch, and is what you would actually use for applying BayesOpt theory to an experiment.\n",
    "\n",
    "Your job will be to read through the code, make sure you understand Ax's syntax and the decisions being made, fill in some missing pieces, and answer some questions at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from ax.plot.contour import plot_contour\n",
    "from ax.plot.trace import optimization_trace_single_method\n",
    "from ax.service.managed_loop import optimize\n",
    "from ax.utils.notebook.plotting import render, init_notebook_plotting\n",
    "from ax.utils.tutorials.cnn_utils import load_mnist, CNN\n",
    "\n",
    "init_notebook_plotting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(29)\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data\n",
    "\n",
    "Ax has a function for loading the MNIST data directly, and calling this function will download the dataset if you are running it for the first time. \n",
    "\n",
    "The dataset is already contained within PyTorch DataLoader objects, and has been split into train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "train_dl, valid_dl, test_dl = load_mnist(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the search space\n",
    "\n",
    "In Ax, a search space is composed of a set of parameters to be tuned in the experiment (and optionally a set of parameter constraints, but we will not be constraining our parameters in this problem). Parameters are defined by their name, parameter type, domain, value type, and some other optional fields. Ax supports three kinds of parameters:\n",
    "\n",
    "* Range parameters -- Domain represented by lower and upper bound\n",
    "* Choice parameters -- Domain is a set of values to choose from\n",
    "* Field parameters -- Domain is a single value\n",
    "\n",
    "The search space is used by Ax's optimization algorithms (Bayesian Optimization for continuous objective functions, Bandit Optimization for problems with a finite set of choices) to know which arms are valid to suggest on a trial. An arm in Ax is a named set of parameters and their values. In our case, an arm is a hyperparameter configuration explored in the course of a given optimization.\n",
    "\n",
    "In the cell below, we define the search space. The parameter for learning rate has already been specified to be a range parameter on \\[1e-6, 0.4\\] on a log scale. It is your job to define the following parameters (none of which will be on log scale):\n",
    "\n",
    "* momentum -- range parameter between 0 and 1\n",
    "* weight_decay -- range parameter between bounds of your choosing\n",
    "* num_epochs -- fixed parameter with a value of 3\n",
    "\n",
    "Note: in this example, we are defining each parameter in the search space as dictionaries, but Ax has other ways to define parameters that correspond to the different Ax APIs. Check out the Ax documentation for more information on the 3 Ax APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_to_optimize = [\n",
    "    {\"name\": \"lr\", \"type\": \"range\", \"bounds\": [1e-6, 0.4], \"value_type\": \"float\", \"log_scale\": True},\n",
    "    # TODO: momentum\n",
    "    # TODO: weight_decay\n",
    "    # TODO: num_epochs\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the train and evaluation functions for the CNN\n",
    "\n",
    "The training function is called once per iteration (trial) of the Bayesian/Bandit optimizer. The training function is just like a normal PyTorch training loop, but the function takes in a set of parameters that have been generated by an acquisition function, and uses these to create the CNN's optimizer for this trial.\n",
    "\n",
    "The model evaluation function takes in a trained model, evaluates it on the validation set, and outputs an overall score for the model. Ax's optimizer is trying to maximize this score as a function of the parameters (lr, momentum, weight decay).\n",
    "\n",
    "Although I am not requiring you to define these functions, make sure to read through them and understand what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_loader, parameters, dtype, device):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        net: initialized neural network\n",
    "        train_loader: DataLoader containing training set\n",
    "        parameters: dictionary containing parameters to be passed to the optimizer\n",
    "        dtype: torch dtype\n",
    "        device: torch device\n",
    "    \"\"\"\n",
    "    net.to(dtype=dtype, device=device)\n",
    "    net.train()\n",
    "    criterion = torch.nn.NLLLoss(reduction=\"sum\")\n",
    "    optimizer = torch.optim.SGD(\n",
    "        net.parameters(),\n",
    "        lr=parameters.get('lr'),\n",
    "        momentum=parameters.get('momentum'),\n",
    "        weight_decay=parameters.get('weight_decay')\n",
    "    )\n",
    "    num_epochs = parameters.get('num_epochs')\n",
    "    \n",
    "    for _ in range(num_epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(dtype=dtype, device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return net\n",
    "\n",
    "def evaluate(net, data_loader, dtype, device):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.to(dtype=dtype, device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the evaluation function for the hyperparameter optimization\n",
    "\n",
    "This function is passed into Ax's optimizer as the overall evaluation function. The higher the returned value, the better the evaluated arm performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(parameterization):\n",
    "    net = CNN()\n",
    "    net = train(\n",
    "        net=net,\n",
    "        train_loader=train_dl,\n",
    "        parameters=parameterization,\n",
    "        dtype=dtype,\n",
    "        device=device\n",
    "    )\n",
    "    return evaluate(\n",
    "        net=net,\n",
    "        data_loader=valid_dl,\n",
    "        dtype=dtype,\n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the optimization loop\n",
    "\n",
    "Here we pass in our parameters, the evaluation function, and an arbitrary objective name to Ax's optimize function, which runs a pre-defined managed optimization loop. In most cases, you would define your own Bayesian Optimization arm generation strategy, but this example is simple enough that this off-the-shelf loop is sufficient.\n",
    "\n",
    "This pre-defined loop runs 20 trials. For the first 5 trials, arm values are generated using a SOBOL sampler to build a baseline dataset. All the subsequent trials are conducted using Ax's GPEI model, which is a Gaussian Process with the Expected Improvement acquisition function. The GP is fit to the current known data and EI is used to generate new points.\n",
    "\n",
    "The optimize function returns the following:\n",
    "   * best_parameters -- A dictionary in the form \\<parameter name\\>: \\<best value\\>\n",
    "   * values -- means and covariances of the objective\n",
    "   * experiment -- An Ax Experiment object, which keeps track of the whole optimization process and contains the search space, optimization configuration, and other metadata \n",
    "   * model -- An Ax ModelBridge object that can be used to generate new points in the search space\n",
    "\n",
    "Run the optimization loop. This shouldn't take too long. If your parameters were set up properly, this should run without any problems. You can ignore any InputData Warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters=parameters_to_optimize,\n",
    "    evaluation_function=train_evaluate,\n",
    "    objective_name='accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot results\n",
    "\n",
    "Ax has many useful visulation functions built off of plotly. plot_contour creates a contour plot showing the classification accuracy as a function of two hyperparameters. Black squares show points we have actually run, and they are clustered in the optimal region. If these plots are not displaying for you, message me on Slack and I can send you a screenshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render(plot_contour(model=model, param_x='lr', param_y='momentum', metric_name='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render(plot_contour(model=model, param_x='lr', param_y='weight_decay', metric_name='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render(plot_contour(model=model, param_x='momentum', param_y='weight_decay', metric_name='accuracy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we plot the objective value as a function of the iteration to visualize the improvement as our BayesOpt model converges to better and better hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_objectives = np.array([[trial.objective_mean*100 for trial in experiment.trials.values()]])\n",
    "best_objective_plot = optimization_trace_single_method(\n",
    "    y=np.maximum.accumulate(best_objectives, axis=1),\n",
    "    title=\"Model performance vs. # of iterations\",\n",
    "    ylabel=\"Classification Accuracy, %\",\n",
    ")\n",
    "render(best_objective_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the final CNN\n",
    "\n",
    "Finally, we retrieve the best arm from the experiment, combine our train and validation sets into a single DataLoader, and train the CNN using the hyperparameters from the best arm. We then evaluate the trained model and see that it did extremely well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = experiment.fetch_data()\n",
    "df = data.df\n",
    "best_arm_name = df.arm_name[df['mean']==df['mean'].max()].values[0]\n",
    "best_arm = experiment.arms_by_name[best_arm_name]\n",
    "best_arm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = torch.utils.data.ConcatDataset([\n",
    "    train_dl.dataset.dataset,\n",
    "    valid_dl.dataset.dataset\n",
    "])\n",
    "\n",
    "combined_dl = torch.utils.data.DataLoader(\n",
    "    combined_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_net = train(\n",
    "    net=CNN(),\n",
    "    train_loader=combined_dl,\n",
    "    parameters=best_arm.parameters,\n",
    "    dtype=dtype,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = evaluate(\n",
    "    net=final_net,\n",
    "    data_loader=test_dl,\n",
    "    dtype=dtype,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Classification Accuracy (test set): {round(test_accuracy*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Followup Questions\n",
    "\n",
    "## Question 1\n",
    "\n",
    "Which part of this entire tutorial seemed the most confusing to you?\n",
    "\n",
    "## Question 2\n",
    "\n",
    "Do you see anywhere in the optimization policy that could be improved? If so, what would you change?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
